{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ============================\n",
    "# 1) LOAD & PREP DATA\n",
    "# ============================\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cosmicclassifierTraining.csv\")\n",
    "target_col = \"Prediction\"\n",
    "features = [col for col in df.columns if col != target_col]\n",
    "\n",
    "# Separate features and target\n",
    "X = df[features].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "# One-hot encode categorical columns (if any)\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "if len(cat_cols) > 0:\n",
    "    X = pd.get_dummies(X, columns=cat_cols)\n",
    "\n",
    "# Impute missing values with mean strategy\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Encode target variable if needed\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and validation sets (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_imputed, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE to balance classes on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Validation shape:\", X_val.shape)\n",
    "\n",
    "# ============================\n",
    "# 2) DEFINE OPTUNA OBJECTIVE FUNCTION (BAYESIAN OPTIMIZATION)\n",
    "# ============================\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space for LightGBM\n",
    "    param = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": len(np.unique(y)),\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500, step=50),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150, step=10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50, step=5),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0, step=0.1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0, step=0.1)\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    \n",
    "    # Use 3-fold cross-validation on the training set\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "# ============================\n",
    "# 3) RUN OPTUNA STUDY\n",
    "# ============================\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "print(\"  Accuracy: {:.4f}\".format(study.best_trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# ============================\n",
    "# 4) TRAIN FINAL LIGHTGBM MODEL WITH BEST HYPERPARAMETERS\n",
    "# ============================\n",
    "final_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": len(np.unique(y)),\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": 42\n",
    "}\n",
    "final_params.update(best_params)\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**final_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# ============================\n",
    "# 5) EVALUATE THE FINAL MODEL ON THE VALIDATION SET\n",
    "# ============================\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "prec = precision_score(y_val, y_val_pred, average=\"macro\")\n",
    "rec = recall_score(y_val, y_val_pred, average=\"macro\")\n",
    "f1 = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\nFinal Validation Metrics:\")\n",
    "print(\"Accuracy:  {:.4f}\".format(acc))\n",
    "print(\"Precision: {:.4f}\".format(prec))\n",
    "print(\"Recall:    {:.4f}\".format(rec))\n",
    "print(\"F1 Score:  {:.4f}\".format(f1))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(\"lightgbm1_model.pkl\",'wb') as f:\n",
    "    pickle.dump(final_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
